#!/bin/sh
#SBATCH --job-name=mega_array   # Job name
#SBATCH --nodes=1                   # Use one node
#SBATCH --ntasks=1                  # Run a single task
#SBATCH --mem-per-cpu=100gb           # Memory per processor
#SBATCH --gres=gpu:1                # Use GPU
#SBATCH --time=03:58:00             # Time limit hrs:min:sec
#SBATCH --output=array_%A-%a.out    # Standard output and error log
#SBATCH --array=1-357                # Array range
 
# This is an example script that combines array tasks with
# bash loops to process many short runs. Array jobs are convenient
# for running lots of tasks, but if each task is short, they
# quickly become inefficient, taking more time to schedule than
# they spend doing any work and bogging down the scheduler for
# all users.

module purge
module load GCC/6.4.0-2.28  OpenMPI/2.1.2
module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130
module load Python/3.6.4
source ~/tf2env/bin/activate
export TF_CPP_MIN_LOG_LEVEL=2 
 
 
#Set the number of runs that each SLURM task should do
PER_TASK=1
 
# Calculate the starting and ending values for this task based
# on the SLURM task and the number of runs per task.
START_NUM=$(( ($SLURM_ARRAY_TASK_ID - 1) * $PER_TASK + 1 ))
END_NUM=$(( $SLURM_ARRAY_TASK_ID * $PER_TASK ))
 
# Print the task and run range
echo This is task $SLURM_ARRAY_TASK_ID, which will do runs $START_NUM to $END_NUM
 
# Run the loop of runs for this task.
for (( run=$START_NUM; run<=END_NUM; run++ )); do
    echo This is SLURM task $SLURM_ARRAY_TASK_ID, run number $run
    #Do your stuff here
    # python /mnt/home/storeyd3/Documents/PyFiles/blind_bfactor/feature_compiler.py $run
    # python /mnt/home/storeyd3/Documents/PyFiles/blind_bfactor/predictor.py $run
    python /mnt/home/storeyd3/Documents/PyFiles/blind_bfactor/CNN_predictor.py $run
    # python /mnt/home/storeyd3/Documents/PyFiles/blind_bfactor/tenfold_CNN_predictor_8.py $run

done